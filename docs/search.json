[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "postgresql\n\n\ndatabase\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreprex\n\n\ncore essential\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nggmap\n\n\nmap\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nweb scraping\n\n\nrvest\n\n\ncore essential\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bradford Johnson",
    "section": "",
    "text": "Data analyst and recent graduate of the Thinkful Data Analytics program. Proficiency with dashboarding, performance tracking, Pandas, Excel, SQL, Tableau, and Python to transform data into meaningful and easily understood visualizations and presentations. Experience in sociology provides an advantage to storytelling with data.\n\n\n\n\n\nThinkful | Data Analytics Bootcamp | June 2022 - December 2022\nWinthrop University | B.A. in Sociology | August 2017 - December 2021\n\n\n\n\n\n\n\n\nTitle\nShort Description\nStatus / Information\n\n\n\n\nVending Machine Analysis\nPython data analysis project with recommendations on vending machine revenue and optimizing stock\n\n\n\nFuel Economy Analysis\nData-backed analysis and recommendations around vehicle fuel economy\n\n\n\nLariat Rent-A-Car Analysis\nBusiness analysis with a focus on minimizing costs and maximizing revenue\n\n\n\nPortfolio Website\nThis is a Quarto website made in R that lists my projects and links to them\n \n\n\nFavorite Pokémon Table\nThis is my first table I made using the gt package for R"
  },
  {
    "objectID": "posts/ggmap/index.html",
    "href": "posts/ggmap/index.html",
    "title": "Making maps with ggmap",
    "section": "",
    "text": "The ggmap package makes life easier when you are trying to visualize geographic data.\nIt connects the ggplot2 framework with mapping services allowing the user to create powerful and insightful maps.\n\n\n\n\nI will be working with crime data from the City of Atlanta Police Department. For this analysis I will use the 2021 crime data and I want to visualize the reported homicides.\n\n\n\nI will be using the tidyverse, ggmap, ggdensity and geomtextpath packages. Below you can view the documentation for the respective package.\n\n\n\nPackage\nDocumentation\n\n\n\n\ntidyverse\ntidyverse.tidyverse.org\n\n\nggmap\nwww.rdocumentation.org/packages/ggmap\n\n\nggdensity\njamesotto852.github.io/ggdensity\n\n\nggtextpath\nwww.rdocumentation.org/packages/geomtextpath\n\n\n\n\n\nIf you are missing any of these package then use this code to install them.\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggmap\")\ninstall.packages(\"ggdensity\")\ninstall.packages(\"geomtextpath\")\nOnce you have everything installed load the packages.\n\nYou have to load the packages each time you use them, but you only have to install them once\n\n\nlibrary(tidyverse)\nlibrary(ggmap)\nlibrary(ggdensity)\nlibrary(geomtextpath)\n\n\n\n\n\n\n\n\nClick here to see all of the available Atlanta crime data\nClick here to download the 2021 Atlanta crime data\n\n\nOnce you have the data you want to use then load it into R.\n\nI downloaded the 2021 Atlanta crime data, cleaned the column names, filtered for only homicide reports, and only kept these columns:\n- offense_id\n- lat\n- long\n\n\n\nMy data is in my working directory named ‘atl-homicides-2021.csv’, I can use the read_csv() function from the readr package (included in tidyverse) and turn the .csv file into a data frame within R. I will name this data frame cobra_df.\n\ncobra_df <- read_csv(\"atl-homicides-2021.csv\")\n\n\n\nNext I will create a simple map that shows the general location of each reported homicide. Each reported homicide location will be marked with a red circle.\n\nmap_1 <- qmplot(long, lat, data = cobra_df,\n                source = \"osm\", maptype = \"roadmap\",\n                color = I(\"red\"))\n\nmap_1\n\n\n\n\n\n\nNow I want to look deeper and create a 2D density plot, using the geom_hdr() function from the ggdensity package allows me to perform 2D density estimation. The geomtextpath package creates the labels, and in this case I am creating labels on the map that correspond to the probabilities for a reported homicide within the respective contoured region.\n\nmap_2 <- qmplot(long, lat, data = cobra_df, geom = \"blank\", \n       zoom = 14, source = \"osm\", maptype = \"roadmap\") +\n  geom_hdr(aes(fill = stat(probs)), alpha = .3) +\n  geom_labeldensity2d(aes(long, lat, level = stat(probs)), stat = \"hdr_lines\") +\n  scale_fill_viridis_d(option = \"A\") +\n  theme(legend.position = \"none\")\n\nmap_2\n\n\n\n\n\n\n\n\n\nWhen I was new to R my favorite learning activity was to try and create crime “heatmaps” with Atlanta’s crime data. Now over 1 year later revisiting this was a good change of pace. I was able to learn about these packages and figure out how to actually extract insights from geographic data (rather than just making cool looking maps).\n\n\n\n\n\n\nFor some insight on how my mapping skills have changed over time\n\nHere is a picture of the first map I created in R on May 5th, 2021.\n\nThis map took many hours to make, and shows Atlanta’s reported homicides from 2009-2020 including some of 2021’s preliminary data.\n\n\n\n\n\nReach out to me if you have any feedback, questions, or suggestions! I am open to collaborating with others on projects and expanding my network! Until next post!"
  },
  {
    "objectID": "posts/postgresql/index.html",
    "href": "posts/postgresql/index.html",
    "title": "PostgreSQL databases and R",
    "section": "",
    "text": "Importing data from a database directly into R for analysis or reporting allows for the optimizing of workflows.\nThe ability to combine SQL and R is huge for extracting insights from your database.\nThere are countless use cases and workflows that can implemented once you pair R and PostgreSQL and I will share one of my recent workflows.\n\nTweet sentiment analysis (project workflow)\n\nIn this workflow, I use various packages and methods to scrape Tweets from Twitter in R. I split the data into two separate data frames, one with more text related Tweet data, the other for the Tweet metadata. I gave each data frame a primary key, for identification purposes, and for future SQL joins. Next I save the two data frames of scraped data in a PostgreSQL database. I can add more data or I could move on to the analysis step\n\n\n\nWhen I move on to the analysis stage I first connect to my database and then I can begin executing SQL queries, which are returned as data frames in R, meaning I can immediately use the results of a query and visualize them with ggplot2 or continue cleaning and wrangling the data with packages like tidytext or dplyr."
  },
  {
    "objectID": "posts/postgresql/index.html#how-to-connect-to-a-postgresql-database-with-r",
    "href": "posts/postgresql/index.html#how-to-connect-to-a-postgresql-database-with-r",
    "title": "PostgreSQL databases and R",
    "section": "How to connect to a PostgreSQL database with R",
    "text": "How to connect to a PostgreSQL database with R\n\nPackages\nYou will need the DBI, RPostgres, and dplyr packages. dplyr comes with the tidyverse meta package so make sure it is updated if you already have tidyverse.\nHere are the packages and links to their documentation websites:\n\n\n\nPackage\nDocumentation\n\n\n\n\nDBI\nhttps://dbi.r-dbi.org/\n\n\nRPostgres\nhttps://rpostgres.r-dbi.org/\n\n\ndplyr\nhttps://dplyr.tidyverse.org/\n\n\n\nYou can use this code to install these packages:\ninstall.packages(\"DBI\")\ninstall.packages(\"RPostgres\")\ninstall.packages(\"dplyr\")\nOnce you install the packages you can go ahead and load the libraries in a R script, or in a R chunk in R Markdown or Quarto.\nlibrary(DBI)\nlibrary(RPostgres)\nlibrary(dplyr)\n\n\n\nConnecting to a database\nAfter loading the packages you need to tell them what database to connect to, and your credentials to connect. Here is what that code looks like, and you just have to fill in your credentials for each argument.\n# establish connection with postgres data base\ncon <- dbConnect(RPostgres::Postgres(),dbname = 'db_name', \n                 host = 'address', # i.e. 'ec2-54-83-201-96.compute-1.amazonaws.com'\n                 port = 5432, # or any other port specified by your DBA\n                 user = 'username',\n                 password = 'password')\nThe database you connect to does not have to be “online”, as I connect to a local PostgreSQL database that I set up on my personal machine. Here is what the code looks like when you connect to a local PostgreSQL database, and don’t mind my password :)\ncon <- dbConnect(RPostgres::Postgres(),dbname = 'postgres',\n      host = 'localhost',\n      port = 5432,\n      user = 'postgres',\n      password = 'RIsBetterThanPython')\n\n\n\nHow to create tables\nYou have created the connection parameters for your PostgreSQL database in R but now lets interact with the database.\nThe dbWriteTable() function allows you to essentially “export” R data frames to your database, without even opening up pgadmin4.\nHere is an example of code that creates tables in PostgreSQL from R\n# create tables in database\ndbWriteTable(con, \"rocketleague_data\", df1, append = TRUE)\ndbWriteTable(con, \"rocketleague_text\", df2, append = TRUE)\nIf you looked at my Tweet sentiment analysis workflow, then this may seem familiar. But this code is calling the dbWriteTable() function, with 4 arguments:\n\nArgument 1 - the connection parameters\nArgument 2 - name to give table in pgadmin4\nArgument 3 - name of the data frame in R\nArgument 4 - append = TRUE\n\n[when TRUE, if an existing table with the name exists it will keep the existing records and add these additional records]\n\n\n\n\n\nHow to execute SQL queries\nWhen I create my SQL queries, I typically create them in pgadmin4 using the query tool first. I do this to make sure my query works as intended, and is optimized. I then copy and paste it into my dbSendQuery() function template which is shown below.\nres <- dbSendQuery(con, \"\n# paste sql here\n                   \n                   \")\nquery_df <- dbFetch(res)\nThe template shows the dbSendQuery() function with two arguments:\n\nArgument 1 - the connection parameters\nArgument 2 - SQL query in quotes\n\nThis function is then saved as an object called res\nThe next function is dbFetch(), which executes a SQL statement on the database connection. This function has the object res as it’s sole argument and it execute the SQL statement and return a data frame in R called query_df.\nHere is an example query:\nres <- dbSendQuery(con, \"\nSELECT DISTINCT(t.id_str), text, display_text_range, retweet_count, favorite_count\nFROM rocketleague_text AS t\nINNER JOIN rocketleague_data AS d\nON d.id_str = t.id_str\n-- DISTINCT(t.id_str) eliminates duplicates \n                   \n                   \")\nquery_df <- dbFetch(res)\n\nFinding out I was able to connect to PostgreSQL databases with R was not only very cool, but eye opening because of the implications.\nThe ability to combine powerful data tools like this can allow you flexibility.\n\nIf I want to do a quick query, then I can use the query tool in pgadmin4.\nBut if I want to gather more strategic insights, clean / wrangle the data, create visuals with packages like ggplot2, or work in RStudio- I have that option.\n\n\n\nContact me\nReach out to me if you have any feedback, questions, or suggestions! I am open to collaborating with others on projects and expanding my network! Until next post!"
  },
  {
    "objectID": "posts/reprex/index.html",
    "href": "posts/reprex/index.html",
    "title": "Creating reproducible examples with reprex",
    "section": "",
    "text": "A reprex, or reproducible example, is an easier way to share code with others in a “reproducible” way.\nAll you need to do after making a reprex is paste it into GitHub, Stack Overflow, etc.\n\nRomain François came up with the term in this tweet:\n\n\nCheck out this quick video that shows the process of making and sharing a reprex, it is super easy to do!"
  },
  {
    "objectID": "posts/reprex/index.html#how-to-make-and-use-reprexes",
    "href": "posts/reprex/index.html#how-to-make-and-use-reprexes",
    "title": "Creating reproducible examples with reprex",
    "section": "How to make and use reprexes",
    "text": "How to make and use reprexes\n\nInstall the reprex package, if you have tidyverse installed then you already have reprex so can skip this step\n\nInstall reprex only\n\ninstall.packages(\"reprex\")\n\nOr install the tidyverse meta package + reprex package\n\ninstall.packages(\"tidyverse\")\n\nHave code that you want to share to others, and even allow them to reproduce\n\nHere is the code that I want to reproduce:\n\nlibrary(dplyr)\nlibrary(gt)\nlibrary(broom)\n\nmtcars %>%\n  do(tidy(t.test(.$mpg ~ .$am))) %>%\n  gt()\n\nThis code will use the mtcars data, included in RStudio, and conduct a t-test between cars with automatic transmissions (0) and cars with manual transmissions (1)\nThe results of the t.test() are then shown in the format of a table for easy viewing\n\n\nLoad in the reprex package\nlibrary(reprex)\n\nSelect and copy CTRL + C all the code you want to make into a reprex\n\nIn the RStudio console run:\nreprex()\n\nPaste and share your reprex!\n\nHere is mine:\n\n\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(gt)\n#> Warning: package 'gt' was built under R version 4.1.3\nlibrary(broom)\n#> Warning: package 'broom' was built under R version 4.1.3\n\nmtcars %>%\n  do(tidy(t.test(.$mpg ~ .$vs))) %>%\n  gt()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n-7.940476\n16.61667\n24.55714\n-4.667053\n0.0001098368\n22.71576\n-11.46251\n-4.418445\nWelch Two Sample t-test\ntwo.sided\n\n\n\nCreated on 2022-11-15 with reprex v2.0.2"
  },
  {
    "objectID": "posts/reprex/index.html#making-a-good-reprex",
    "href": "posts/reprex/index.html#making-a-good-reprex",
    "title": "Creating reproducible examples with reprex",
    "section": "Making a good reprex",
    "text": "Making a good reprex\n\n\nCheck out the reprex do’s and don’ts to get familiar with reprex etiquette.\nClick the hex image to view the reprex documentation"
  },
  {
    "objectID": "posts/rvest/index.html",
    "href": "posts/rvest/index.html",
    "title": "Web scraping with rvest",
    "section": "",
    "text": "Using the rvest package you can get the data you need from webpages for analysis.\nBelow I will show a simple script using the rvest and tidyverse packages that can scrape us some data from Wikipedia about the Seinfeld original television soundtrack.\nFirst you will need to install and load these packages, if you already have any of these packages then you just need to load them."
  },
  {
    "objectID": "posts/rvest/index.html#install-and-load-packages",
    "href": "posts/rvest/index.html#install-and-load-packages",
    "title": "Web scraping with rvest",
    "section": "Install and load packages",
    "text": "Install and load packages\n# install packages\n    install.packages(\"tidyverse\")\n    install.packages(\"rvest\")\n\n# load packages\n    library(tidyverse)\n    library(rvest)\n\nTo see the documentation for rvest click the hex image"
  },
  {
    "objectID": "posts/rvest/index.html#setting-the-parameters",
    "href": "posts/rvest/index.html#setting-the-parameters",
    "title": "Web scraping with rvest",
    "section": "Setting the parameters",
    "text": "Setting the parameters\nNext you will set up the parameters so rvest knows where to get the data from. First I create the object link with the website’s link. After I can use this object as an argument for the read_html() function. This function is then assigned to the object page.\n# link to get data from\n    link <- \"https://en.wikipedia.org/wiki/Seinfeld\"\n\n# read webpage at the above link\n    page <- read_html(link)\nFor each of the columns you want to create you will need to repeat the following steps:\n\n\nCreate object with the column’s name\nUse selector gadget to get the html_nodes for this column\nRepeat the below code as shown changing each object’s name to it’s respective column name, and html_nodes\n\n\nThe html_nodes can be found using the browser extension SelectorGadget found here. Using this extension you can highlight what you want to web scrape and copy/paste the html nodes from SelectorGadget.\nFor my 3 columns: title, episodes, and length this is how the code looks:\n# scrape title\n    title <- page %>%\n        html_nodes(\".tracklist td:nth-child(2)\") %>%\n        html_text()\n\n# scrape episodes\n    episodes <- page %>%\n        html_nodes(\".tracklist td:nth-child(3)\") %>%\n        html_text()\n\n# scrape length\n    length <- page %>%\n        html_nodes(\".tracklist-length\") %>%\n        html_text()"
  },
  {
    "objectID": "posts/rvest/index.html#creating-the-data-frame",
    "href": "posts/rvest/index.html#creating-the-data-frame",
    "title": "Web scraping with rvest",
    "section": "Creating the data frame",
    "text": "Creating the data frame\nAfter getting all the data you will want to put it into a data frame to work with it, to do this use the data.frame() function. Below you will see how I am creating the data frame with the previously made objects.\nI am also using the gsub() function to clean up the title and episodes columns, here I am removing quotes. Because the function takes quotes in it’s first two arguments gsub(\"...\", \"...\", x) a backslash, “\\” is used to tell the function to ignore the next symbol allowing it through. In this case not supplying the backslash would cause an error as the argument within the quotation marks is a quotation mark causing the function break.\n# create df\n    df <- data.frame(title, episodes, length)\n\n# remove quotes from data\n    df$title <- gsub(\"\\\"\", \"\", df$title)\n    df$episodes <- gsub(\"\\\"\", \"\", df$episodes)\nNow lets see the first 6 rows of our new data frame that we crafted using rvest.\n\n# view top 6 records\n    head(df)\n\n                 title              episodes length\n1       Seinfeld Theme                         0:52\n2       Seinfeld Theme The Highlights of 100   0:40\n3       Seinfeld Theme         The Chronicle   0:33\n4 The Jerry Show Theme     The Pilot, Part 2   0:50\n5    Kramer's Pimpwalk        The Wig Master   0:53\n6    Jerry the Mailman      The Andrea Doria   0:35"
  },
  {
    "objectID": "projects/fuel-economy/index.html",
    "href": "projects/fuel-economy/index.html",
    "title": "Fuel Economy",
    "section": "",
    "text": "Statistical Significance | Fuel Economy"
  },
  {
    "objectID": "projects/fuel-economy/index.html#project-summary",
    "href": "projects/fuel-economy/index.html#project-summary",
    "title": "Fuel Economy",
    "section": "Project Summary 🗒️",
    "text": "Project Summary 🗒️\n\n🧭- Project scenario\n\nProvide data-backed analysis and recommendations around vehicle fuel economy\nClient is interested in vehicle types, manufacturers, and technical specs\nThey want to understand how those might drive fuel economy and annual fuel costs\n\n\n\n\n\n\n\nProject Objective\n\n\n\n\nIdentify and conduct statistical tests on factors that drive fuel economy and annual costs\n\n\n\n\n\n📂- Deliverables\nClick the icons or text below to see my project files and deliverables\n\n\n\nPowerPoint\nExcel\n\n\n\n\n\n\n\n\n\n\n\n🔧- Methods\n\n\n\n\n\n\nResearch Questions\n\n\n\n\nDoes the Start / Stop Technology make a significant difference in fuel costs / consumption?\n\n\nDoes the car’s “age” or model year have a significant difference in fuel economy?\n\n\nHow significant is the car’s class (car type: minivan, truck, compact…) in the consumption of fuel?\n\n\n\n\nMost vehicles use regular gas as a fuel type -> focus on regular gas vehicles\n\nRegular gas is convenient, and already has nationwide infrastructure\n\nSubcompact and compact vehicles are the most common classes -> focus on these car classes\n\nThese two classes are the most popular and they are quite similar in specific metrics\n\nCar “age” determined by the model year -> focus on new vs used\n\n\n\n\n🔍- Findings\n\n\nStart Stop Technology - Fuel Cost\n\n\n\nCar Class - Combined MPG\n\n\n\nCar Age - Used Fuel Quantity\n\n\n\n5 Year Savings and Spending\n\n\n\n💡- Recommendations\n\nFor fuel economy and savings, without sacrificing the convenience of regular gas, use cars with Start / Stop Technology\nPrioritize using newer cars (2015 and newer) as on average they have a significantly better combined MPG than older counterparts\nImplementing the above and using the compact vehicle class will offer the best savings in terms of fuel economy"
  },
  {
    "objectID": "projects/lariat-rentals/index.html",
    "href": "projects/lariat-rentals/index.html",
    "title": "Lariat Rentals",
    "section": "",
    "text": "Rental fleet business analysis | Revenue growth model"
  },
  {
    "objectID": "projects/lariat-rentals/index.html#project-summary",
    "href": "projects/lariat-rentals/index.html#project-summary",
    "title": "Lariat Rentals",
    "section": "Project Summary 🗒️",
    "text": "Project Summary 🗒️\n\n🧭- Project scenario\n\nI am consulting as a data analyst for Lariat\nThey hired me to make suggestions on how they can make smarter business decisions\nMy job is to analyze the costs and revenue generated by their rental car fleet\nThey provided me the data for their nationwide, 4,000-car fleet\n\n\n\n\n\nLariat’s Business Objective\n\n\n\n\nMinimizing costs and maximizing revenue\n\n\n\n\n\n\n📂- Deliverables\nClick the icons or text below to see my project files and deliverables\n\n\n\nPowerPoint\nExcel\n\n\n\n\n\n\n\n\n\n\n\n\n🔧- Methods\n\nData cleaning and exploration\nDefining parameters\n\n\nGroup vehicles by model year for baseline\n\nBreak down “the numbers” to the daily level (ex. Revenue per day, cost per day…)\n\n\n\nCreate a user scenario that can take custom values and apply them to the baseline\n\n\n\n\n🔍- Findings\n\nThe top 4 most popular car makes: Ford, Chevrolet, Dodge, and Toyota\n\n\nThese 4 car makes account for over ¼ of the rental fleet\n\n\nThe daily revenue and cost per car does not vary much by model year\n\nTotal rental days is not significantly different among the 3 different car model years\n\n\n\n\n💡- Recommendations\n\nMinimizing Costs\n\nRetire and sell high-cost, low return vehicles\nReplace with more popular and higher revenue make/models\n\nMaximizing Revenue\n\nIncrease the prices for rentals\nWhen doing this assume a decrease in total days rented"
  },
  {
    "objectID": "projects/vending-machines/index.html",
    "href": "projects/vending-machines/index.html",
    "title": "Vending Machine Sales",
    "section": "",
    "text": "Python Data Analysis | Vending Machine Sales"
  },
  {
    "objectID": "projects/vending-machines/index.html#project-summary",
    "href": "projects/vending-machines/index.html#project-summary",
    "title": "Vending Machine Sales",
    "section": "Project Summary 🗒️",
    "text": "Project Summary 🗒️\n\n🧭- Project scenario\n\nUse Python and pandas to explore a dataset and ultimately craft an analysis for a final presentation\nCreate a notebook demonstrating a clear story about the research question, the hypothesis, and the results of testing it\nPresent findings and analysis process with a slide deck\n\n\n\n\n\nDataset Used\n\n\n\n\nVending Machine Sales from Kaggle\n\n\n\n\n\n\n📂- Deliverables\nClick the icons or text below to see my project files and deliverables\n\n\n\nJupyter Notebook\nPowerPoint\nProject Proposal\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔧- Methods\n\nData cleaning and exploration using Python\nPandas, NumPy, seaborn, etc.\n\n\n\n\n\n\nResearch Questions\n\n\n\n\n\nWhat product categories generate the most revenue on average?\n\nHow can the product selection be optimized to increase revenue?\n\n\n\n\n\nWhat vending machine locations generate the most sales?\n\nHow can vending machines be optimized by location?\n\n\n\n\n\n\n\n\n\n🔍- Findings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n💡- Recommendations\n\nOverall\n\nSpace is limited | Optimize the space used and customize the selection based on location\n\nCondense product selection to meet consumer preferences to increase revenue\nBalance selection and variety to lower costs for buying in bulk (economy of scale)\nLocations where people work or spend lots of time tend to have more purchases\n\n\n\n\nLocation Specific\n\nBrunswick Sq Mall\n\nKeep only the ‘ATT’ vending machine\nAllocate the majority of space for food and water options, with the carbonated and non-carbonated options only being for the most popular overall products\n\nEarle Asphalt\n\nMajority of options should be food products\nWater, carbonated and non-carbonated options only being the most popular for this location\n\nGutten Plans\n\nMajority of options should be food and carbonated drink products\nNon-carbonated options should only be the most popular for this location\nTest with the most popular water products to see if this option is in demand\n\nEB Public Library\n\nMajority of vending products should be food\nStock popular carbonated drinks and non-carbonated product options\nOnly the most popular water products should be stocked at this location"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Dec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]